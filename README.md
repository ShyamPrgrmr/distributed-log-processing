# Logs Infrastructure â€“ Docker Compose Setup

This project sets up a **log ingestion and processing pipeline** using Docker Compose.  
It simulates application logs, ships them via Fluentd to Kafka, processes them with Logstash, and finally indexes them into Elasticsearch and Kibana.

---

## ğŸ§± Architecture Overview

**Flow:**

![Architecture Diagram](Flow Diagram.drawio.svg)


---

## ğŸ“¦ Services Included

- **Log Generator** â€“ Generates application logs
- **Fluentd** â€“ Collects logs from shared volume and forwards to Kafka
- **Kafka (KRaft)** â€“ Controller + Broker setup
- **Logstash** â€“ Consumes logs from Kafka and pushes to Elasticsearch
- **Elasticsearch**
  - Coordination node
  - Data node
  - Init container for templates / index setup

---

## âœ… Prerequisites

Make sure you have the following installed:

- Docker `>= 24`
- Docker Compose `>= v2`
- At least **6â€“8 GB RAM** allocated to Docker

---

## ğŸ“ Project Structure

```

.
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ fluentd/
â”œâ”€â”€ logstash/
â”œâ”€â”€ loggenerator/
â”œâ”€â”€ kafka/
â”‚   â”œâ”€â”€ conf/
â”‚   â””â”€â”€ data/
â”œâ”€â”€ elasticsearch/
â”‚   â”œâ”€â”€ coordination_node/
â”‚   â”œâ”€â”€ data_node/
â”‚   â””â”€â”€ elasticsearch-init/

````

---

## ğŸŒ Required Docker Network & Volumes

This setup uses **external volumes and network**, so they must be created manually **before starting**.

### Create Docker Network

```bash
docker network create logs-network
````

### Create Docker Volumes

```bash
docker volume create logs-storage
docker volume create es-data
```

---

## â–¶ï¸ How to Run the Project

### Step 1: Clone the Repository

```bash
git clone https://github.com/ShyamPrgrmr/distributed-log-processing.git
cd distributed-log-processing
```

---

### Step 2: Configure Environment Files

Ensure all `.env` files exist and are properly configured:

* `loggenerator/.env`
* `fluentd/.env`
* `kafka/conf/.controller.env`
* `kafka/conf/.broker.env`
* `logstash/.env`
* `elasticsearch/**/.env`

---

### Step 3: Start the Stack

```bash
docker compose up -d --build
```

This will:

* Build custom images
* Start Kafka (controller first, then broker)
* Bring up Fluentd, Logstash, and Elasticsearch
* Run Elasticsearch init container once

---

### Step 4: Verify Running Containers

```bash
docker compose ps
```

All services should be in **running** state except `es-init` (expected to exit after completion).

---

## ğŸ” Useful Checks

### Kafka Broker

```bash
docker logs kafka-broker-1
```

### Fluentd

```bash
docker logs fluentd
```

### Logstash

```bash
docker logs logstash
```

### Elasticsearch Health

```bash
curl http://localhost:9200/_cluster/health?pretty
```

(If exposed via port mapping)

---

## ğŸ§¹ Stopping the Stack

```bash
docker compose down
```

---

## ğŸ—‘ï¸ Full Cleanup (Optional)

If you want a **fresh start**:

```bash
docker compose down -v
docker volume rm logs-storage es-data
docker network rm logs-network
```

---

## ğŸ“Œ Notes

* Kafka runs in **KRaft mode** (no Zookeeper).
* Fluentd uses a **position file** to handle log rotation safely.
* Elasticsearch data persists across restarts.
* `es-init` runs once to configure index templates.

---

## ğŸš€ Next Improvements (Optional)

* Add Kafka UI (AKHQ / Kafdrop)
* Add Elasticsearch index lifecycle policies
* Scale Kafka brokers

---

## ğŸ§  Maintained By

Shyam Pradhan
```bash
Feel free to fork, experiment, and extend ğŸš€
```
